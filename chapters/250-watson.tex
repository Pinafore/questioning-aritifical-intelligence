

I remember when I first heard rumblings of Watson.
%
Because I had a foot in both the \abr{ai} and trivia communities, I
heard two different stories.
%
I heard rumors of amazing work in parsing and semantic role labeling
happening from researchers who ventured north to Nassau county in New
York (I was doing my PhD at Princeton).
%
From the trivia community, I heard of some people who were being paid
by \abr{ibm} to play trivia games but that they couldn't say anything
more.

I was very sceptical.
%
By the time that Watson came to fruition, I had moved to the
University of Maryland.
%
Then, my scepticism turned to jealousy.
%
I watched, along with the rest of the world, one of the greatest
achievements of \abr{ai} unfold in front of me.
%
What was I doing wasting my time working on topic models if this was
also legitimate research?

Let me be clear that the technical triumphs are indisputable (and, in
my opinion, under-appreciated).
%
From the work on wagering to synthesizing multiple information
sources, Watson~\citep{ferruci-10} was from top to bottom a top-notch well-oiled machine.
%
And it computed all of this in real time---something that wasn't
strictly necessary but still impressive.

After talking a little about how Watson works, I'll get into how
Watsons victory left me feeling unconvinced about the primacy of
computers' ability to answer questions.

\section{How Watson Works}

While ``neural'' question answering (which we'll discuss in the next
chapter) is the big thing these days, Watson came of age in the
\emph{statistical} age.
%
To understand some of how Watson works, its helpful to review some of
the work that came before Watson that helped inspire it.

Preceeding the statistical age of \abr{qa} was the \emph{rule-based}
age: systems that were impressive on individual questions about
baseball or geology but that faltered as soon as it saw a question
that was unexpected in terms of the domain, they'd fail miserably.

One of the guiding principles of the statistical age was ``the
unreasonable effectiveness of big data'', and this required scaling 
approaches to web-scale data.
%
The first things that people tried was scaling up the ``good,
old-fashioned'' approaches that defined the rule-based systems.
%
Probably the most prominent system in this category is Start from Boris Katz at MIT.  This system first launched in 1993.  This like many of the systems in this era, take an approach that’s somewhat similar to to the old-fashioned AI approaches: parse the query with a  and look it up in a knowledge base.  And since this was in the age of the semantic web, this was in the form of RDF files.  

But this is more a change in scale rather than in form from LUNAR and
BASEBALL\dots but still an amazing achievement to deal with the
messiness of Internet text.  And its ability to \emph{search} the
Internet to find answers is clearly a huge influence on later
techniques that we’ll see like Watson today and Dense Passage
Retrieval later (Chapter~\ref{ch:nq}).

A more transformative approach was the \abr{qasm} approach from Radev
at al.  It used the concepts from machine learning to find answers
given a query.  In the information theory interpretation of machine
translation, you imagine that all of the utterances in a target
language is a corruption of English.  Now this is pretty complicated
for machine translation, but this is a little bit like what happened
with how the the game show \jeopardy{} came to be.

In Chapter~\ref{ch:dynabench}, we'll talk about the cheating scandal
around the American game show \textit{21}, where Charles van Doren got
the answers in advance.
%
Part of the mythology of Jeopardy! is that Merv Griffen said, why
don’t we just give them the answers!
%
So you have clues like “During
WWII this computer scientist \& code breaker converted his money into
silver \& buried it; he never found his buried treasure”.  The correct
response, which is phrased as a question, is “Who is Alan Turing?”
%
In other words, the central conceit of Jeopardy! is that it converts questions into answers (and then the contestants need to give a response to that answer in the form of a question).

What \abr{qasm} is doing is doing the reverse: it has a model that
tries to turn questions into the search string that could find the
answer to the question.
%
There are a number of transformations that it applies: swapping words, deleting words, ignoring words (e.g., if you have Cleveland, Ohio and ignore Ohio, you’ll find more things about US president Grover Cleveland), etc.

In the end, you have a model that creates specialized queries from the
original question.
%
Later on, when we talk about dense passage
retrieval, those systems are doing something similar, except they’re
generating the query in a continuous vector space.
%
Doing it with real
words like QASM is more interpretable: a human could understand what’s
going on, while a vector query is inscrutable it seems to work better.

Okay, enough delay.  Let’s talk about \jeopardy{} and how Watson
answers \jeopardy{} clues.  First, Jeopardy! doesn’t have one kind of
clue.  There are many different types.  Let’s start with rhyme time:
from the name of the category, you know that both words in the
response have to rhyme with each other.
%
This is in essence a
constrained search: ``tramp stamp'' you could find with a single search,
but other times you need to do two searches and try to find the thing
that rhymes.

In other cases, the constraint is that the correct answer starts with a particular letter.  

The category that I feared the most was anagrams: where you need to rearrange the letters in the clue to get to the answer.  This is actually easier for a computer than a human!

Each of these can have different approaches that can generate guesses that might be the correct answer.  And you might think that this is all encoded in the category.  Not so!  

And for “normal” clues, this looks a lot like the reformulations that we saw in QASM: 400th anniversary of 1898 gets rewritten to 27th May 1498, India gets transformed into Kappad, and then you can find something with the correct response: Vasco de Gama.

So the first phase of the Watson pipeline is to—in parallel—generate all sorts of guesses based on different interpretations of the question.  Some try to solve anagrams, others look for rhymes, others run an IR query like the Start system we talked about before, others are doing transformations like the QASM approach we talked about before.  

And sometimes the approaches need to take a recursive approach, combining different subsystems to get to the right answer.

From this you have dozens of possible guesses.  How do you know which—if any—to select?  This all gets fed into a logistic regression problem.  This can take into account how consistent the evidence matches the clue, how popular the response is, and it can even take into account things like Jeopardy!’s love of puns.

In my homework code, I call this component the buzzer.  And this is the same function that the logistic regression function plays here.  If the logistic regression is confident enough, Watson buzzes in to answer.

\section{How Does Watson Know when to Buzz?}

If you’re watching this video, I’m assuming you know both about logistic regression and a little bit about IBM’s Watson that played the US game show Jeopardy!  If you want to know more, I have some suggestions for videos linked into the description.

The most interesting part of what Watson did to answer questions was to estimate its confidence: how likely is guess a to clue c going to be correct.  It gathered a bunch of individual clues together to figure this out.  

But how did Watson actually put this information together?  It used a tool called logistic regression.  The goal of this approach is to take all of the information about a possible guess and output a probability of how likely it thought the guess would be correct.  The higer the score, the more it trusted the guess.

There are many little pieces that could go into thinking that a guess is good or not.  These pieces of evidences are called features.  While features aren’t always active, when they are, they provide evidence of whether a guess is good.  For example, you could have a feature (and Watson probably did) for when a category has a quote in it.

In a computer programming language, this could simply be an if statement: if ASCII character 34 is in this string, return 1.  But just knowing that there’s a quote in the category does’t tell you which clues to favor.  You would need to make the the feature more specific.

Let’s go to a question that I got while I was on Jeopardy!  And this shows that these aren’t just a category-specific phenomenon: .  However, this can also apply to individual clues as well.  In a quite normal category about geography, this clue came up:

"G.I." hope you know that 0 degrees latitude \& 180 degrees longitude is just east of this group, part of Kiribati

Notice that “G.I.” is in quotation marks.  This is a signal, that the correct response will start with G.I., in this case the correct response is “Gilbert Islands”.   Named of course for Johnny Gilbert, “the voice of Jeopardy!”.  Actually not, it’s far more confusing than that: it was named by a German admiral—Adam Johann von Krusenstern—who led the first Russian circumnavigation of the globe.  Krusenstern recognized that the British Captain Thomas Gilbert had described each of the islands individually and applied the the name to the group of islands.

So perhaps we need to have multiple features to capture whether our answer is consistent with this clue.  To be clear, we don’t know exactly what features Watson used, but it could look something like this.  You might have one feature to indicate that there is a quotation mark in the clue.  You could have another feature to show that the quote matches the candidate guess.  Perhaps you have another feature that indicates whether the feature matches a multiword guess.

But these are just answering yes or no questions … how does the system know how to balance these different features to come to a final decision?  At this point, you need to look at the cases where the system has generated a lot of guesses and seen which of them are correct or not.  From this, you can learn that when a letter is in quotes and it matches, that’s a good sign.  These are called feature weights and for the binary features that we’ve talked about, they’re either on or off.

But not all of the features need to be binary.  Another very good feature might be how well the guess matches the clue (e.g., cosine similarity between representations).  So a way to generalize both the binary features and continuous features is to take the dot product of a vector representing all of the features and all of the weights to compute a score for the guess.  The higher the score, the better the guess and the more likely the system will answer.

This is not a one and done process.  If you're building a system like
this, you should look at things the system is still getting wrong and
add a new feature to correct the issue.  This is the same process that
Watson used to build their statistical system.  Maybe the system got
confused when somebody has a quote from literature and you need to
make sure that doesn’t get counted or add a new feature to handle that
case.  It’s very rare to get the features right the first time around.

The statistical approach has some advantages over the neural approach
we'll see in the next chapter: it is easy to understand why a system
is doing what it is.  And it's easy to fix problems as they pop up.

\section{Learning as You Go}

Something that is particularly unique to Jeopardy! and not to the
other QA settings we’ve looked at is that the category is a huge
constraint on what the correct responses are.  A large part of the
Jeopardy! buzzer is figuring out the “lexical answer type”.  Part of
this is is looking at clues in the question, things like “this
country” or “this game” tell you what kind of entity the response
would be.  And this is sometimes at odds with the category, as shown
here.


And part of what made Watson a good Jeopardy! Player is that it would learn as it explored the category.  After getting a couple of months wrong, Watson can learn that all of the answers should be a month.

One thing that made Watson particularly impressive was that it not just computed raw accuracy but also compared against human performance.  

This chart showed how Watson progressed in different iterations of the system, inching up to the cloud of Jeopardy! champions.  Ken Jennings is in red there, still clearly dominating even the final version of Watson.

\section{This Game is Rigged, I Tell Ya!}

These successes have been well documented (not least by \abr{ibm}
itself, who rightly celebrated the great achievements); however,
things were not perfect\dots the game was rigged.
%
It's useful to go over the lifecycle of an entire question: how it was
written, how it's communicated, how players answer, and how the game
unfolds afterward.
%
At every stage, there's a slight benefit to the computer, which taken
together makes this an unfair competition.

This is a problem!  First, it's a problem scientifically because we
want to have fair comparisons of human vs. computer intelligence.
%
More importantly, I want to have my turn having my question answering
robots face off against trivia whizzes (Chapter~\ref{ch:game-show}),
and I can't do that if everybody thinks that Watson's spin
on \jeopardy{} settled the issue (and it hasn't).

% Cite / read this:
% https://dominoweb.draco.res.ibm.com/reports/rc25356.pdf

But first, in case you don't know how \jeopardy{} works, we'll review
that.
%
However, if you've calculated a Coryat score before, you can go ahead
and skip ahead.

\subsection{What \jeopardy{} is and How it Works}

\jeopardy{} is a gameshow created by Merv Griffin that debuted in 1967~\citep{griffin-03}.
%
Its big gimmick is that the player responses are given in the form of
a question in reaction to infamous cheating scandals.
%
For example, the clue
\question{
  The CAPTCHA test against spam \& robot programs
  is called the `reverse' test named for this British code breaker
}
would have response \answer{Who is Alan Turing}.
%
% Nevertheless, we'll still sometimes call responses answers to be
% consistent with the other chapters.
%
The clues are arranged in a grid: columns represent categories and
rows represent difficulty, with the more difficult questions being
worth more.

There are three players who stand side by side behind podiums.
%
When a clue is read, any of the three players can ``buzz in'' to say
that they want to give a response.
%
If they give the correct response, they then have ``control of the
board'' and can select the next clue.

One advantage of controlling the board is that some questions are
called ``Daily Doubles'' which allow the player to potentially double
their score: a player can wager any part of their score, and if they
get it right they get that ammount added to their score (but a wrong
response will subtract it from their score).

\subsection{The Pool of Questions}

\jbgcomment{Citation needed}

Part of the agreement between \jeopardy{} and \abr{ibm} was that the
competition would take place on normal, written questions.
%
In the media coverage of the competition, this focused on avoiding
video and picture daily doubles (fairly reasonable, but we'll discuss
this more in a bit).
%
However, this causes two problems: the questions are too easy and do
not necessarily challenge computers.

So what makes up ``normal'' questions?
%
Every game of \jeopardy{} has questions that range in difficulty.
%
Because it's a television show, many questions are easy enough that
the average viewer at home can get them.
%
Moreover, the humans on the stage with Watson are not normal contestants.
%
Ken Jennings is certifiably the greatest of all
time~\citep[\abr{goat}]{low-20} \jeopardy{} player, and Brad Rutter
isn't bad himself.

The average ``normal'' \jeopardy{} contestant, including not so great
players like yours truly, know a large majority of the clues.
%
For top players like Brad and Ken, they know---with a handful of
exceptions--\emph{all} the clues.
%
In a one-on-one fight with normal clues, Ken and Brad would be
fighting over every clue: it would come down to who could buzz first.

This isn't fun to play.
%
Nor is it fun to watch.
%
This is why \jeopardyp{}'s tournament of champions is played on much
more difficult clues~\citep{harris-06}.
%
Nonetheless, this is the battlefield where Watson won: ``normal''
questions that didn't challenge the human players.
%
Instead, it all came down to the buzzer.

\subsection{John Henry vs. the Buzzing Machine}

Unlike \qb{} (Chapter~\ref{ch:qb}), while \jeopardy{} also uses
signaling devices, these only work \emph{once the question has been
  read in its entirety}; Ken Jennings (also a former \qb{} player while he was a student at
\abr{byu}) himself explains it on a \textit{Planet Money}
interview~\citep{malone-19}:
\begin{quote}
{\bf Jennings:} The buzzer is
    not live until Alex finishes reading the question. And if you buzz
    in before your buzzer goes live, \emph{you actually lock yourself out
    for a fraction of a second}. So the big mistake on the show is
    people who are all adrenalized and are buzzing too quickly, too
    eagerly. \\
{\bf Malone:} \abr{ok}. To some degree, \jeopardy{} is kind of a video game, and a \emph{crappy video game where it's, like, light goes on, press button}---that's it. \\
{\bf Jennings:} (Laughter) Yeah. \\
\end{quote}
\jeopardy{}'s buzzers are a gimmick to ensure good television; however, \qb{} buzzers discriminate knowledge.

So how does this interact with Watson?
%
Watson receives all of the clues electronically and likewise gets an electronic signal to know when it is safe to buzz in.
%
In contrast, humans have to either guess when it is safe to buzz or
wait for a light to turn on.

\jbgcomment{citation needed for gurus}

\jeopardy{} gurus explicitly advise new players not to wait for the light---your puny human reflexes are too slow.
%
Indeed, one of Ken Jenning's strengths was his uncanny ability to
internalize the cadence of Alex's voice and when a technitian would
activate the buzzer~\citep{jennings-06}.
%
In contrast, Watson is literally an electromechanical buzzing machine
that could get first crack at every question it wants.\footnote{In
practice, this is not always true.  Because Watson computed its
responses in real time, it could not come up with a response in time
for particularly short questions.}

Unfortunately, despite subjecting everyone within earshot to these
rants, the computer science community thinks that the question is
settled: computers are better than humans at answering questions.
%
This is despite Ken basically saying that it did, indeed, come down to
the buzzer.

Moreover, what makes for a ``normal'' difficulty question for a human
does not always apply to a computer.
%
Let's first talk about what makes a clue easier for a computer and
then we'll talk about what makes a clue harder for a computer.

\paragraph{Easy for a Computer.}

When I appeared on \jeopardyp{}, my final \jeopardy{} was:
\question{ After this woman's death, her daughter wrote, ``As far as we
  in the family are concerned, the alphabet now ends at Y''}
%
All of us got the question right; it just so happened that \jeopardy{}
used a very similar clue that aired as we were recording:
%
\question{``G'' is for grand master as well as this woman who received the 2009 Grand Master Award.}
%
The correct response is of course \answer{Sue Grafton}.
%
For poorly read contestant like myself, only studying previous clues
allowed us to get the answer right.
%
I've never read a Grafton book, but I know she writes mystery books and
has titles of the form \textit{``A'' is for Alibi} (and that's the
only title I can think without looking at Wikipedia).

For Watson, this is ``memorization'' is trivial: a single letter
implies that the answer is \underline{Grafton}.
%
But just like you should not think that I'm smart for getting lucky to
have seen a reused clue, you should not praise Watson for finding
near-repeat questions.
%
And it's not just trivia games: Google's dataset of questions (which
we'll talk about in more detail the next chapter) have
many identical questions~\citep{lewis-21}.
%
Moreover, Watson can also store the entirety of Wikipedia, easily
looking up capitals, authors, etc.

Indeed, when a computer can find \emph{an exact quote} (as was in my
final \jeopardy{} clue), the question becomes even easier.
%
Then the computer just needs to find the appropriate article that
contains the quote and then just find whatever entity is mostly likely
to be a \jeopardyp{} answer.

Where systems like Watson struggle are on computation, matching novels
and movies to plots, combining multiple clues, lateral thinking, and
wordplay~\citep{kaushik-18}.
%
And this is not just a matter of degree: computers struggle with \emph{all}
such questions, even if they're in the top row of \jeopardyp{}.
%
While a computer is theoretically good at math, the kinds of programs
that answer trivia questions struggle answering match questions with
numbers in the double digits~\citep{wallace-19:numbers}.

This is why the goal of \abr{ai} is \emph{general} artificial
intelligence (Chapter~\ref{ch:turing}): while we can build specialized
systems for \jeopardy{} questions or math questions, unlike a
reasonably smart human, a single program can't ``do it all''.
%
Unlike for human contestants, the ``difficulty'' of \jeopardy{}
questions for a computer has no relationship to the nominal value.
%
We talk about how Facebook/Meta dealt with this problem in
Chapter~\ref{ch:dynabench}: tell the authors what's hard for a computer.


\subsection{Two Nice Guys, One Computer with no Shame}

Given the ``too easy'' questions and the buzzer, what does this
actually mean for gameplay?
%
A question comes in, and Watson has the choice of answering it or not:
it can win every race to the buzzer if it wants.
%
Then, of the things it cannot answer, Ken and Brad fight over the
scraps.
%
Thus, for a computer to win this competition, it needs only to be able
to answer a third of the questions correctly.

Now, the \jeopardy{} nerds reading the book (I love you all), will
point out that this isn't true, because the clues are not weighted
equally: some are worth more than others.
%
However, as we discussed above, what's difficult for a computer isn't
always difficult for a human (and \textit{vice versa}), so it really
is a random third of the questions.
%
While a good human player might be weak on the buzzer and be confident
that if they know more they'll win the harder clues, this isn't true
for a human facing off against Watson.

Moreover, the computer has no shame: it uses a strategy called the
``Forrest Bounce''~\citep[more infamously associated with James Holzhauer
and Arthur Chu]{rogak-20}.
%
Rather than going through the categories top to bottom (easy to hard),
Watson goes through the clues somewhat randomly, searching for Daily
Doubles and trying to optimize its score.
%
Again, there's nothing wrong with this---it's the optimal strategy!
%
But if it's the ``right'' way to play the game, why doesn't every
human do it?

That's because humans want to follow social norms.
%
The producers of the show (hi, Maggie and Corinna!) tell you up and
down that you shouldn't play the game like that, and you don't want to
make them unhappy with you\dots they can make your life miserable.
%
I remember watching \jeopardy{} with my grandmother and when someone
hunted for the Daily Double, she would always say ``who does he think
he is'' (and it was always a he).
%
Alex Trebek also wasn't a
fan~\citep{marchese-18}:\footnote{\citet{rogak-20} quotes a saltier
  take Trebek offered to Howard Stern: ``It only works, dickweed, if
  you know the correct response to everything that's up there.''}
%
\begin{quote}
  When the show's writers construct categories they do it so that
  there's a flow in terms of difficulty, and if you jump to the bottom
  of the category you may get a clue that would be easier to
  understand if you'd begun at the top of the category and saw how the
  clues worked. I like there to be order on the show, but as the
  impartial host I accept disorder.
\end{quote}
%
And nobody, nobody, wants to make Alex unhappy.

Ken would sometimes do a little hunding for the Daily Double against a
particularly formiddable opponent, but he would normally be
well-behavied so as not to upset the powers that be.
%
Watson, however, was a soulless machine; and having a machine on the
stage was no exciting that
nobody faulted it for its strategy.
%
If anyone is to blame, it's probably Gary Tesauro; adding his
strategy for playing the board increased Watson's win percentage
considerably~\citep{tesauro-13}.
%
But if you put him in a room with the withering stares of \jeopardy{}
producers (or worse, Alex Trebek) and that code would be deleted in no
time.

% TODO(jbg): Cut substantially

\section{The Legacy of Watson}

% TODO (jbg): Add good stuff --- Tesauro, LAT, retriever--reader pipeline

Let's review Watson's appearance on \jeopardyp{}:
\begin{enumerate*}
        \item all questions are of ``normal'' difficulty;
        \item thus the two human contestants know nearly all of the clues; but
        \item Watson can win the buzzer race whenever it wants.
\end{enumerate*}
If Watson wins such a match, does that mean that it is superior to
these supurb humans?

I hope that you are reluctant to answer ``yes'' (not just
because \jeopardy{} has trained you to respond to answers with
questions).
%
Perhaps I've planted a seed of doubt: \emph{no, we cannot yet conclude
  computer superiority} from this experiment.
%
While many might want to believe that Watson is the end of
human--computer question answering competitions, it's only the
beginning.
%
Watson was just the beginning of the story, a story that unfolds
further in the next few chapters.

While \jeopardy{} and quiz bowl are the domain of trivia nerds, what
happens when computer scientists set the stage to decide whether
humans or computers are smarter?
%
Do humands stand a chance on \emph{that} battlefield?
