
% NOTE: This chapter develops the Manchester paradigm and its evaluation norms.
While Chapter~\ref{ch:ir} introduced Ellen Voorhees's Cranfield paradigm as a
way of testing systems' ability to answer information-seeking questions, this
chapter introduces an alternative---the Manchester paradigm. You have not
heard of it before because we are making it up.

This chapter explains why we picked the name---symmetry with Cranfield and a
nod to question answering mythology---and argues why the Manchester paradigm is
often more effective than the Cranfield paradigm. This will matter for the
serious business of leaderboards (Chapter~\ref{ch:leaderboards}). But until
then, this chapter spends time on fun, silly trivia games played by college
nerds and the complicated rules they have made up over the years, which help
us determine how smart an \abr{ai} is.

\section{Two British University Towns Alike in Question Answering}
\label{sec:manchester:name}

Question answering inherits two evaluation traditions. The Cranfield paradigm
prioritizes serving users (as discussed in Chapter~\ref{ch:ir}). In this
chapter, we introduce a new alternative---the Manchester paradigm---which
prioritizes probing intelligence through designed questions. We keep the
Cranfield discussion light here and focus on why the Manchester framing matters
for evaluation.

\subsection{Why the Name Manchester}
\label{sec:manchester:why-name}

We choose ``Manchester'' for symmetry with ``Cranfield,'' but the name is also
substantive. It names a paradigm that treats questions as probes of
intelligence rather than requests for information. The name also points to
three strands of question answering history: teaching, comparing, and probing.

\paragraph{To Teach: The Sphinx and the Socratic Method}
Manchester's Regiment used the Sphinx as its symbol~\cite{farmer-01}, and the
Sphinx riddle tradition underscores questions that test the answerer more than
they inform the asker (Chapter~\ref{ch:civilization:civilization}).
In Greek myth, the Sphinx asks everyone who enters the city a riddle that is
explicitly designed to test the answerer~\cite{renger-13}. The point of the
riddle is not information seeking; it is to teach and reveal understanding.
This aspect of question answering connects to the Socratic method, where
questions guide the answerer to insight~\cite{trepanier-17}. The Manchester
paradigm adopts this stance: questions are instruments for teaching and
probing, not just for retrieving facts.

\paragraph{To Compare: Granada Studios and \textit{University Challenge}}
Granada Studios, home of \textit{University Challenge}, made a competitive,
discriminative style of question answering famous~\citep{taylor_mcnulty_meek,baber-15}.
The host knows the answer. The goal is to compare answerers, not to satisfy a
user's information need. This competitive framing foreshadows our Quizbowl
discussion in Section~\ref{sec:manchester:qb} and motivates the leaderboard
analogies in Section~\ref{sec:manchester:tournaments}.

\paragraph{To Probe: The Turing Test}
Alan Turing's imitation game reframes intelligence as a question-driven
evaluation problem (Chapter~\ref{ch:turing}). Rather than define intelligence,
Turing proposes an indistinguishability test~\citep{Turing-95}. That move is
quintessentially Manchester: it uses questions to probe whether a machine can
match a human's capabilities. This logic underlies modern \abr{qa} challenges
that are designed to reveal specific strengths and weaknesses rather than to
serve users directly.

\subsection{What the Manchester Framing Adds}
\label{sec:manchester:framing}

The Manchester paradigm emphasizes questions designed to probe for specific
capabilities. This does not replace user-centered evaluation; instead it makes
explicit that some datasets and leaderboards are better understood as tests of
competence rather than proxies for user satisfaction.

\section{Your Trivia Tournament is a Leaderboard}
\label{sec:manchester:tournaments}

The \qa{} community is obsessed with evaluation.
Schools, companies, and newspapers hail new \abr{sota}s and topping
leaderboards, giving rise to troubling claims~\citep{lipton-19} that an
``\abr{ai} model tops humans''~\citep{najberg-18} because it `won'
some leaderboard, putting ``millions of jobs at
risk''~\citep{cuthbertson-18}.
But what is a leaderboard?
A leaderboard is a statistic about \qa{} accuracy that induces a ranking over participants.

Newsflash: this is the same as a trivia tournament.
The trivia community has been doing this for decades~\citep{jennings-06}.
The key difference is that the trivia community has learned hard lessons about
fairness, clarity, and discrimination, and those lessons should inform how we
design and interpret \qa{} leaderboards.

If we treat a leaderboard as a tournament, we should also treat the test set
as sacred. Repeated probing---even when you do not see the answers---can leak
information through the score itself. Computers have infinite patience: you
can flip a low-confidence answer, see whether the score rises or falls, and
learn something about what was right. This is not hypothetical; it is the same
logic that motivates budgeted comparisons and careful reporting of model
selection.

These concerns are not new. The Manchester framing makes them unavoidable:
if a leaderboard is a competition, we must do what competitions do to be fair.
This will be important for the serious business of leaderboards
(Chapter~\ref{ch:leaderboards}), where we formalize what it means for a
leaderboard to discriminate between systems.

Section~\ref{sec:tournament} details this overlap between the qualities of a
first-class \abr{qa} dataset (and its requisite leaderboard).
The experts running these tournaments are imperfect, but they have learned from
their past mistakes (see Appendix~\ref{sec:history} for a brief historical
perspective) and created a community that reliably identifies those best at
question answering.
Beyond the format of the \emph{competition}, trivia norms ensure individual
questions are clear, unambiguous, and reward knowledge (Section~\ref{sec:manchester:craft}).

We are not saying that academic \abr{qa} should surrender to trivia questions
or the community---far from it.
The trivia community does not understand the real-world information-seeking
needs of users or what questions challenge computers.
However, they have well-tested protocols to declare that someone is better at
answering questions than another.
This collection of tradecraft and principles can nonetheless help the \abr{qa}
community.

Beyond these general concepts that \abr{qa} can learn from,
Section~\ref{sec:manchester:qb} reviews how the ``gold standard'' of trivia
formats, \qb{}, can improve traditional \abr{qa}.
We then briefly discuss how research that uses fun, fair, and good trivia
questions can benefit from the expertise, pedantry, and passion of the trivia
community (Section~\ref{sec:call}).

The trivia community learned its evaluation norms the hard way. In the 1950s,
the popularity of television quiz shows led to high-profile scandals, including
rigged outcomes on \emph{Twenty One} and the notoriety of Charles van
Doren.\jbgcomment{TODO: find real cites \url{https://youtu.be/JRKxNpwqBac?t=1154} \url{https://youtu.be/bj-m3Ddmn0E?t=83}}
The lesson was simple: do not cheat. The implication for \abr{qa} evaluation is
less obvious, but the logic is the same. If a system can repeatedly probe a
test set and use the score as feedback, then information leaks from the test
set even without revealing the answers.

This leakage can be subtle. A system can flip a low-confidence answer and see
whether the score increases or decreases, effectively learning which answers
are right. With enough attempts, and with correlations across questions, a
system can improve its score without improving its underlying ability. This is
why careful evaluation requires a budgeted comparison process and a clear
separation between development and test data.

Two further lessons follow. First, evaluation should reward knowledge rather
than luck or speed. This is why trivia formats emphasize discriminative
questions and why a leaderboard should be interpreted as a competitive
comparison rather than a casual statistic. Second, good evaluation requires
good questions. Crafting such questions is a skill, and it is one that the
trivia community has refined for decades.

There are, of course, tradeoffs. Human tournaments are not perfect, and
embarrassing losses happen. We suffered a particularly public loss in 2016
against expert quiz bowl players.\footnote{\url{https://www.youtube.com/watch?v=c2kGD1EdfFw}}
That is not an argument against rigorous evaluations. If anything, it is a
reminder to evaluate more frequently and with fresh questions. It is also a
reason to use surprise questions in courses and system evaluations, where a
single attempt better reflects real-world use.

These points motivate the next sections, which move from test-set sanctity to
the structure of questions themselves.

\section{Quiz Bowl}
\label{sec:manchester:qb}

\subsection{Quizbowl Lineage and the \textit{University Challenge} Link}
\label{sec:manchester:qb-lineage}

Quizbowl is the clearest embodiment of the Manchester paradigm in practice.
Its mythology traces to a USO diversion, often credited to Don Reid, and its
modern format was shaped by radio host Allen Ludden. It shares a family
resemblance with \textit{University Challenge}: both are competitive,
knowledge-centered formats in which the asker already knows the answer, and the
goal is to compare answerers rather than to serve a user. This is the
``compare'' motivation that motivates the Manchester paradigm.

\subsection{Discriminative Questions and Incremental Evidence}
\label{sec:manchester:qb-discriminative}

Quizbowl's defining property is interruptibility: questions are read aloud and
can be answered at any time. Each word is a chance to demonstrate knowledge.
This makes Quizbowl maximally discriminative, rewarding knowledge rather than
buzzer speed or luck. It is also a better model for evaluating \abr{qa} than
formats where answers are only permitted after the question is fully read, as
in \jeopardy{}.\footnote{\jeopardy{} is still a canonical comparison task, as
  in \abr{ibm} Watson's match against Ken Jennings~\citep{ferrucci2010building}.}

This matters because leaderboards should discriminate between systems. In
earlier work, we argued that an ideal \abr{qa} dataset maximizes the proportion
of discriminative questions. Quizbowl operationalizes that idea by using
incremental evidence within each question.

We have shown example \qb{} questions, but have not explained the format; see
\citet{DBLP:journals/corr/abs-1904-04792} for more. You might be scared off by
how long the questions are, but in real \qb{} tournaments they are not finished
because the questions are \emph{interruptible}.

\paragraph{Interruptible}

A moderator reads a question. Once someone knows the answer, they use a
signaling device to ``\emph{buzz in}.'' If the player who buzzed is right, they
get points; otherwise, they lose points and the question continues for the
other team.

Not all trivia games with buzzers have this property. In \jeopardy{}, the
buzzer is not live until the question has been read in its entirety. Ken
Jennings explains the consequence in a \textit{Planet Money} interview:
\begin{quote}
{\bf Jennings:} The buzzer is not live until Alex finishes reading the
    question. And if you buzz in before your buzzer goes live, \emph{you
    actually lock yourself out for a fraction of a second}. So the big mistake
    on the show is people who are all adrenalized and are buzzing too quickly,
    too eagerly. \\
{\bf Malone:} \abr{ok}. To some degree, \jeopardy{} is kind of a video game,
and a \emph{crappy video game where it's, like, light goes on, press button}---that's it. \\
{\bf Jennings:} (Laughter) Yeah. \\
\end{quote}
\jeopardy{}'s buzzers are a gimmick to ensure good television; \qb{} buzzers
discriminate knowledge (Section~\ref{sec:discriminative}). Similarly, while
\triviaqa{}~\citep{joshi-17} is written by knowledgeable writers, the questions
are not pyramidal.

\paragraph{Pyramidal}
\label{sec:manchester:pyramidality}

Recall that effective datasets discriminate the best from the rest---the higher
the proportion of effective questions~$\rho$, the better. \qb{}'s~$\rho$ is
nearly 1.0 because discrimination happens \emph{within} a question: after every
word, an answerer must decide if they know enough to answer. \qb{} questions
are arranged to be maximally \emph{pyramidal}: questions begin with hard clues
that require deep understanding and proceed to more accessible clues.

\subsection{Adjudication and Protests as Quality Control}
\label{sec:manchester:qb-adjudication}

Format is necessary but not sufficient. Quizbowl works because the community
invests in question quality and adjudication. In the early 1990s, the Academic
Competition Federation (\abr{acf}) formalized norms for clarity, specificity,
and fair evaluation. Their rules are designed to avoid ambiguity and to make
answer expectations explicit (e.g., when a day and month are required). This
aligns with modern critiques of ambiguity in \abr{qa} datasets.

The community also developed protest and adjudication norms that recognize
legitimate answer variants. That is directly relevant to machine \abr{qa}:
accepting only one surface form (e.g., ``Timothy Donald Cook'' but not ``Tim
Cook'') is both unfair and counterproductive. As we discuss in
Section~\ref{sec:manchester:craft}, answer correctness is often about
equivalence, not exact matching.

For example, a \jeopardy{} contestant was initially ruled incorrect for
answering \underline{endoscope} to ``Your surgeon could choose to take a look
inside you with this type of fiber-optic instrument.'' After a protest and
review, judges ruled the response acceptable because \underline{endoscope} is a
more general term that includes \underline{laparoscope}.\footnote{\url{http://www.j-archive.com/showgame.php?game_id=6112}}
This is not a pedantic exception; it is a model of quality control that
acknowledges legitimate equivalence.

\paragraph{Well-Edited}

\qb{} questions are created in phases. First, the \emph{author} selects the
answer and assembles (pyramidal) clues. A \emph{subject editor} then removes
ambiguity, adjusts acceptable answers, and tweaks clues to optimize
discrimination. Finally, a \emph{packetizer} ensures the overall set is
diverse, has uniform difficulty, and is without repeats.

\paragraph{Unnatural}
\label{sec:manchester:unnatural}

Trivia questions are fake: the asker already knows the answer. But they are no
more fake than a course's final exam, which---like leaderboards---is designed
to test knowledge. Experts know when questions are ambiguous
(Section~\ref{sec:manchester:ambiguity}); while ``what play has a character
whose father is dead'' could be \textit{Hamlet}, \textit{Antigone}, or
\textit{Proof}, a good writer's knowledge avoids the ambiguity. When authors
omit these cues, the question is derided as a ``hose''~\citep{2013-eltinge},
which robs the tournament of fun (Section~\ref{sec:fun}).

One of the benefits of contrived formats is a focus on specific phenomena.
\citet{dua-19} exclude questions an existing \abr{mrqa} system could answer to
focus on challenging quantitative reasoning. One of the trivia experts
consulted in \citet{wallace-19} crafted a question that tripped up neural
\abr{qa} by embedding the phrase ``this author opens Crime and Punishment'' into
a question; the top system confidently answers \underline{Fyodor Dostoyevski}.
However, that phrase was in a longer question ``The narrator in \textit{Cogwheels}
by this author opens \textit{Crime and Punishment} to find it has become
\textit{The Brothers Karamazov}''. Again, this shows the inventiveness and
linguistic dexterity of the trivia community.

A counterargument is that real-life questions---e.g., on Yahoo!
Questions~\citep{szpektor-13}, Quora~\citep{iyer-17} or web
search~\citep{kwiatkowski-19}---ignore the craft of question writing. Real
humans react to unclear questions with confusion or divergent answers,
explicitly answering with how they interpreted the original question (``I
assume you meant\dots''). Given real-world applications must cope with this
noise, our systems must cope with it too. However, addressing the real world
cannot happen by glossing over its complexity.

\paragraph{Complicated}

\qb{} is more complex than other datasets. Unlike other datasets where you just
need to decide \emph{what} to answer, in \qb{} you also need to choose
\emph{when} to answer the question.\footnote{This complex methodology can be an
  advantage. The underlying mechanisms of systems that can play \qb{} (e.g.,
  reinforcement learning) share properties with other tasks, such as
  simultaneous translation~\citep{grissom:he:boyd-graber:morgan-2014,ma-etal-2019-stacl},
  human incremental processing~\citep{levy-08,levy-11}, and opponent
  modeling~\citep{he-16}.}
While this improves the dataset's discrimination, it can hurt popularity
because you cannot copy/paste code from other \abr{qa} tasks. The cumbersome
pyramidal structure complicates\footnote{But does not necessarily preclude, as
  the Illinois High School Scholastic Bowl Coaches Association shows:
\begin{quote}
    This is the smallest counting number which is the radius of a sphere whose volume is an integer multiple of $\pi$. It is also the number of distinct real solutions to the equation $x^7-19x^5=0$. This number also gives the ratio between the volumes of a cylinder and a cone with the same heights and radii. Give this number equal to the log base four of sixty-four.
\end{quote}} some questions (e.g., what is log base four of sixty-four).

\subsection{The Case for Yearly Blind Test Sets}
\label{sec:manchester:qb-blind}

National Academic Quiz Tournaments (\abr{naqt}) professionalized these norms
and scaled them to large tournaments. Their goal is not only to write
questions, but to create efficient systems for determining which of many teams
knows the most. That requires both a discriminative format and a rigorous
answer-acceptability process.

Machine \abr{qa} has not yet matched this standard. Human competitions are
replenished continuously with new questions written and edited by domain
experts, while \abr{qa} leaderboards often reuse stale data written by
crowdworkers or harvested from the web. Even dynamic datasets such as
Dynabench, or our own TrickMe interface, are small compared to the tens of
thousands of questions written each year for human tournaments.

The natural remedy is a yearly blind test set. Systems should be evaluated once
on a fresh, hidden test set; then the questions and answers should be revealed,
and protests should be allowed. This would move \abr{qa} leaderboards closer to
the standards of human competitions and reduce the leakage problems discussed
earlier.

\section{The Craft of Question Writing}
\label{sec:manchester:craft}

Trivia enthusiasts agree that questions need to be well written (despite other disagreements).
Asking ``good questions'' requires sophisticated pragmatic reasoning~\citep{hawkins-15}, and pedagogy explicitly acknowledges the complexity of writing effective questions for assessing student performance~\citep[focusing on multiple choice questions]{Haladyna-04}. 

\abr{qa} datasets, however, are often collected from the wild or written by untrained
crowdworkers.
Crowdworkers lack experience in 
crafting questions and may introduce idiosyncrasies that shortcut machine learning~\citep{geva-19}.  
Similarly, data collected from the wild
such as Natural Questions~\citep{kwiatkowski-19} or AmazonQA~\citep{gupta-19} by design have vast variations in quality.
In the previous section, we focused on how datasets as a whole should be structured.
Now, we focus on how specific \emph{questions} should be structured to make the dataset as valuable as possible.


\subsection{Avoiding ambiguity and assumptions}
\label{sec:manchester:ambiguity}

Ambiguity in questions not only frustrates answerers who resolve the ambiguity `incorrectly'.
Ambiguity also frustrates the goal of using questions to assess knowledge.
Thus, the \abr{us} Department of Transportation explicitly bans ambiguous questions from exams for flight instructors~\citep{dot-08}; and the trivia community has likewise developed rules and norms that prevent ambiguity.
While this is true in many contexts, examples are rife in format called \qb{}~\citep{boyd-graber-12}, whose very long questions\footnote{Like \jeopardy{}, they are not syntactically questions but still are designed to elicit knowledge-based responses; for consistency, we still call them questions.} showcase trivia writers' tactics.
For example, \qb{} author Zhu Ying (writing for the 2005 \abr{parfait} tournament) asks participants to identify a fictional character while warning against possible confusion [emphasis added]:
\begin{quote}
 He's {\bf not Sherlock Holmes}, but his address is 221B. He's {\bf not the Janitor on Scrubs}, but his father is played by R. Lee Ermy. [\dots] For ten points, name this misanthropic, crippled, Vicodin-dependent central character of a FOX medical drama. \\
{\bf ANSWER:} Gregory \underline{House}, MD
\end{quote}

In contrast, \qa{} datasets often contain ambiguous and
under-specified questions.
While this sometimes reflects real world complexities such as actual
under-specified or ill-formed search
queries~\citep{faruqui-18,kwiatkowski-19}, ignoring this ambiguity is
problematic.
As a concrete example, Natural Questions~\citep{kwiatkowski-19} answers ``what year did the us hockey team win the Olympics'' with \underline{1960} and \underline{1980}, ignoring the \abr{us} women's team, which won in 1998 and 2018, and further assuming the query is about \emph{ice} rather than \emph{field} hockey (also an Olympic event).
Natural Questions associates a page about the United States men's national ice hockey team, arbitrarily removing the ambiguity \textit{post hoc}.
However, this does not resolve the ambiguity, which persists in the original question: information retrieval arbitrarily provides one of many interpretations.
True to their name, Natural Questions are often under-specified when users ask a question online.

The problem is neither that such questions exist nor that machine
reading \abr{qa} considers questions given an associated context.
The problem is that tasks do not explicitly acknowledge the original
ambiguity and gloss over the implicit assumptions in the data.
This introduces potential noise and bias (i.e., giving a bonus to systems that make
the same assumptions as the dataset) in leaderboard rankings. 
At best, these will become part of
the measurement error of datasets (no dataset is perfect). 
At worst, they will recapitulate the biases that went into the creation of the datasets.
Then, the community will implicitly equate the biases with correctness: you get high scores if you 
adopt this set of assumptions.
These enter into real-world systems, further perpetuating the bias.
Playtesting can reveal these issues (Section~\ref{sec:fun}), as implicit assumptions 
can rob a player of correctly answered questions.
If you wanted to answer \underline{2014} to ``when did Michigan last win the championship''---when the Michigan State Spartans won the Women's Cross Country championship---and you cannot because you chose the wrong school, the wrong sport, and the wrong gender,
you would complain as a player; researchers instead discover latent assumptions that creep into the data.\footnote{Where to draw the line is a matter of judgment; computers---which lack common sense---might find questions ambiguous where humans would not.}

It is worth emphasizing that this is not a purely hypothetical problem. For example, Open Domain Retrieval Question Answering~\citep{lee-19} deliberately avoids providing a reference context for the question in its framing but, in re-purposing data such as Natural Questions, opaquely relies on it for the gold answers.

\subsection{Avoiding superficial evaluations}

A related issue is that, in the words of \citet{voorhees-00}, ``there is no such
thing as a question with an obvious answer''.
As a consequence, trivia question authors 
delineate acceptable and unacceptable answers.

For example, in writing for the trivia tournament Harvard Fall XI, Robert Chu uses a mental model of an answerer to explicitly delineate the range of acceptable correct answers:
\begin{quote}
     In Newtonian gravity, this quantity satisfies Poisson's equation. [\dots] For a dipole, this quantity is given by negative the dipole moment dotted with the electric field. [\dots] For 10 points, name this form of energy contrasted with kinetic.\\
    {\bf ANSWER:} \underline{potential energy} \textit{(prompt on energy; accept specific types like electrical potential energy or gravitational potential energy; do not accept or prompt on just ``potential'')}
\end{quote}

Likewise, the style guides for writing questions stipulate that you
must give the answer type clearly and early on.
These mentions specify whether you want a book, a collection, a movement, etc.  
It also signals the level of specificity requested.  
For example, a question about a date must state ``day and month required'' (\underline{September 11}, ``month and year required'' (\underline{April 1968}), or ``day, month, and year required'' (\underline{September 1, 1939}).
This is true for other answers as well: city and team, party and
country, or more generally ``two answers required''.
Despite these conventions, no pre-defined set of answers is perfect,
and every worthwhile trivia competition has a process for adjudicating
answers.

In high school and college national competitions and game shows,
if low-level staff cannot resolve the issue by throwing out a single
question or accepting minor variations (\underline{America} instead of
\underline{\abr{usa}}), the low-level staff contacts the tournament
director.
The tournament director, who has a deeper knowledge of rules and questions, often decide the issue.
If not, the protest goes
through an adjudication process designed to minimize
bias:\footnote{\smallurl{https://www.naqt.com/rules/\#protest}}
write the summary of the dispute,
get all parties to agree to the summary,
and then hand the decision off to mutually agreed experts from the tournament's phone tree.
The substance of the disagreement is communicated (without identities), and the experts apply the rules and decide.

Consider what happened 
when a particularly inept \jeopardy{} contestant\footnote{\smallurl{http://www.j-archive.com/showgame.php?game_id=6112}} did not answer \underline{laproscope} to ``Your surgeon could choose to take a look inside you with this type of fiber-optic instrument''.
Since the van Doren scandal~\citep{freedman-97}, every television trivia contestant has an advocate assigned from an
auditing company.
In this case, the advocate initiated a process that went to a panel of judges who then
ruled that \underline{endoscope} (a more general term) was also correct.

The need for a similar process seems to have been well-recognized
in the earliest days of \qa{} system bake-offs such as \abr{trec-qa}, and \citet{voorhees-08} notes that
\begin{quote}
    [d]ifferent \abr{qa} runs very seldom return exactly the same [answer], and it is quite difficult to determine automatically whether the difference [\dots] is significant.
\end{quote} 
In stark contrast to this, \qa{} datasets typically provide only a single
string or, at best, a short list of strings. A correct answer then means
\emph{exactly} matching these strings or having a high token overlap
\fone{}. That framing treats answer equivalence as string similarity, but
humans routinely accept paraphrases, aliases, and answers with the right level
of specificity. The experiments we and others have run make the same point at a
high level: exact matching underestimates correctness, while purely semantic
matching can overestimate it. In other words, answer equivalence is not a
single number; it is a set of rules and adjudications that depend on context.

There is a practical reason the community clings to exact match: it is fast,
transparent, and easy to reproduce. But it also bakes in a narrow view of
correctness that modern \abr{qa} systems regularly violate by producing valid
paraphrases or more specific answers. Neural matching and \abr{llm}-based
judges help, but they can be inconsistent across datasets, over-accept near
misses, and are costly to run at scale. The upshot is not that we should
replace one metric with another, but that correctness needs a rubric.
The trivia community already has one. Rules from \abr{naqt} and
\textit{Jeopardy!} formalize when aliases are acceptable, when specificity is
required, and when extra detail is disqualifying~\citep{naqt_correctness,carberry_jeopardy_casebook}.
These rubrics make explicit the judgments that automated evaluations leave
implicit, and they highlight why correctness is about interpretation, not just
overlap.

\abr{naqt} is a thirty-year-old purveyor of \abr{qa} competitions spanning
middle schools to ``open'' competitions that anyone can join.
%
Because of the breadth of participants and judges (often drawn from parent
volunteers of participants), the organization has developed easy-to-follow
rules that make answer adjudication consistent (otherwise, participants would
complain about unfairness).
%
However, \abr{naqt} and \textit{Jeopardy!} formats are for
\emph{human} players, we cannot unquestioningly adopt the rules for machine
\abr{qa}.
%
For example, some rules cover in-person delivery of answers: mispronouncing
\underline{koan} as \underline{cone}, pauses between parts, etc.
%
These and other rules are irrelevant to text-based \abr{qa}.

%
% Likewise, the American game show \textit{Jeopardy!}---previously the setting of \abr{ibm}'s \textit{tour-de-force} \abr{qa}.

% \jbgcomment{I think I disagree with some of this.  Sure, pronunciation isn't applicable, but foreign language names I think is still certainly relevent, as is name order.  ``Li Zongxia'' is acceptable, but ''Graber Boyd Jordan'' is not.  So I think I'd just focus on pronunciation.


% \paragraph{Experts also play a role in machine \abr{qa} paradigm}
%

%


% \jbgcomment{I think rather than giving one concrete example here, I'd list all of the aspects of a correct answer here and forward point to the appendix where you can go into more detail.  I think we can also format the appendix guidelines more nicely, as right now it's the same as we used for a gpt prompt}

On the other hand, the rubrics nonetheless provide rules we can adopt.
%
For example, the \textit{specificity} rule is
both present in the \abr{naqt} and \textit{Jeopardy!} rules, where the
responses should be specific enough under the context of a
question---\textit{Question: Where is the Eiffel Tower located?}---where the
answer \textit{\uline{Europe}} is incorrect if the given \refer{} answer is
\textit{\uline{France}}, but is acceptable if \textit{\uline{Europe}} were the
intended answer.


\begin{table}[t] % This specifies that the table should appear at the top of the page
\centering
\tiny
\renewcommand{\arraystretch}{1.5} % Increase row spacing
\begin{tabular}{p{2cm}p{4.8cm}} % Adjust the column widths to fit one column of the page
\hline
\textbf{Rule} & \textbf{Description} \\ \hline
$R_1$: Entity-aliasing & Widely recognized aliases, pseudonyms that are commonly associated with referred answer entities are acceptable. \\ 
$R_2$: Numerical information & Exact dates, years, numerical values are required unless the question specifically asks for approximations. \\ 
$R_3$: Less details & The answer provides less detail but should include the essential and correct information required by the question (specificity level 1). \\
$R_4$: More details & The answer contains additional information that does not contradict the question or initial answer (specificity level 2). \\
$R_5$: Semantic equivalence & A high degree of word overlap does not establish equivalence. The answer must be contextually and semantically accurate. \\
$R_6$: Irrelevant information & Any irrelevant or inaccurate description related to the question should be considered incorrect. \\
$R_7$: Other possible answers & The response is correct but not in the initially provided list. \\
\hline
\end{tabular}
\caption{The correctness of a \can{} answer can be traced and categorized to
  one or more of the above rules. We adopt the rules from \abr{naqt} modified
  after analysis of an annotated dataset with human correctness judgments
  between \refer{} answers and machine generated answers. The acceptability of
  \abr{qa} model answers are based on the rubrics. Rules with
  examples are in Table~\ref{tab:more_ae_examples} (Appendix).}

% \ishanicomment{In order to justify the adoption of these rules, we can also provide examples and a very short description of the rules, as was done by the Tomahto, Tomato paper? Something like your Figure 1 in Appendix}
\label{tab:guidelines}
\end{table}

At a high level, recent experimental work in answer equivalence reaches the
same conclusion: exact match consistently underestimates correctness, while
embedding-based or \abr{llm}-based judges can overestimate it, especially on
short or factual answers~\citep{bulian2022tomayto,kamalloo-etal-2023-evaluating,wang2023evaluating}.
The most reliable evaluations align with human judgments but are too expensive
to use everywhere~\citep{verga2024replacingjudgesjuriesevaluating,kim2024prometheus}.
That is why importing Manchester-style rules is so useful: they give us a
shared definition of correctness even when the metric is imperfect.

Our goal is not to minimize the achievements of strong systems or the value of
quantitative evaluations. The point is more modest: \emph{blind automation} is
not enough to distinguish between systems on a leaderboard. If we want
evaluations that reward knowledge rather than artifacts, we need to borrow the
adjudication mindset of the trivia community.

Taking our cue from the trivia community, we present an alternative
for \abr{mrqa}.
Blind test sets are created for a specific time; all systems are submitted simultaneously.
Then, all questions and answers are revealed.
System authors can protest correctness rulings on questions, directly
addressing the issues above. 
After agreement is reached,
quantitative metrics are computed for comparison purposes---despite their inherent limitations they at least can be
trusted.
Adopting this for \abr{mrqa} would require creating a new, smaller test
set every year.
However, this would gradually refine the annotations and process.

This suggestion is not novel: \citet{voorhees-00} accept automatic
evaluations ``for experiments internal to an organization where the
benefits of a reusable test collection are most significant (\emph{and
  the limitations are likely to be understood})''~(our emphasis) but
that ``satisfactory techniques for [automatically] evaluating new
runs'' have not been found yet.  We are not aware of any change on
this front---if anything, we seem to have become more insensitive as a
community to just how limited our current evaluations are.

\subsection{Focus on the bubble}

While every question should be perfect, time and resources are limited.  
Thus, authors and editors of tournaments ``focus on the bubble'', where the ``bubble'' are the questions most likely to discriminate between top teams at the tournament.
These questions are thoroughly playtested, vetted, and edited.
Only after these questions have been perfected will the other questions undergo the same level of polish.

For computers, the same logic applies.  
Authors should ensure that these discriminative questions are correct, free of ambiguity, and unimpeachable.
However, as far as we can tell, the authors of \qa{} datasets do not give any special attention to these questions.

Unlike a human trivia tournament, however---with finite patience of the participants---this does not mean that you should necessarily remove all of the easy or hard questions from your dataset.
This could inadvertently lead to systems unable to answer simple questions like ``who is buried in Grant's tomb?''~\cite[Chapter 7]{dwan-00}.
Instead, focus more resources on the bubble.

\section{Ignore the Distinction at Your Peril}
\label{sec:manchester:distinction}

\paragraph{How Adversarial Is Too Much?}
%
Common ground has its limits. That there is not a strict dichotomy between
Manchester and Cranfield can mask the importance of distinguishing motivations.
Some robustness proposals test models on inputs users would not make, or on
challenging but unnatural adversarial questions~\citep{feng2018rawr,jia2017adversarial,wallace2018trick,bartolo2020beat,kiela2021dynabench}.
These tests can be valuable for probing capabilities, but they do not directly
improve the user experience in the short term. The Manchester paradigm
welcomes these probes; the Cranfield paradigm will often ignore them. If we do
not state which goal we are pursuing, we risk building datasets that satisfy
neither.

\paragraph{Users Are the Customers}
%
The transition from Manchester-style spectacle to Cranfield-style product is
hard. \abr{ibm} Watson's \jeopardy{} victory was a tour de force of the
Manchester paradigm, but turning that success into a user-facing product was
much more difficult~\citep{chu2002piquant,deutscher-21,jennings2011ibm}. The
same tension shows up in dataset design. \squad{} questions were written by
people who knew the answers, but the data do not follow Manchester-style
question-writing norms; the result was a proliferation of shortcuts and
artifacts that later analyses exposed~\citep{weissenborn2017simple}. In
contrast, Manchester-oriented benchmarks such as the Winograd challenge
explicitly try to avoid cheap tricks~\citep{levesque2014behavior}. The point is
not that one paradigm is superior, but that each requires different care.

\paragraph{Comparisons Require Fewer Questions}
%
Competitions are designed to discriminate between answerers quickly. The world
accepted that Watson was better than Ken Jennings and Brad Rutter after 122
answers because the format is optimized to separate abilities. Cranfield-style
leaderboards, in contrast, require thousands of questions and still may not
have enough statistical power to meaningfully rank systems~\citep{card2020power,rodriguez2021leaderboard}.
This is another place where answer equivalence matters: if your evaluation
cannot recognize legitimate variants, you need more data to compensate for the
noise. The Manchester paradigm forces us to confront this issue directly by
building questions and evaluations that make discriminations clear.
