
\clearpage

\paragraph{Proposed Title and Subtitle} \vspace{3cm}

{\bf Questioning Artificial Intelligence}: What the Differences between how
Computers and Humans Answer Questions reveal about Intelligence and an
AI-dominated Future.

\vspace{3cm}

Humans have been answering questions for millenia: \OE{}dipus answering the
Sphynx's riddles, civil servants proving their worth on exams, and contestants
on \jeopardyp{}.
%
But now computers are doing the same thing: answering idle questions on smart
devices, competing on game shows, and threatening to take our jobs.
%
This book looks at how---despite the superficial similarity in the end
result---dramatically different the underlying mechanisms for human and
computer question answering are and what that says about the future of
human--computer coexistance.

\paragraph{Full Description}

We call someone smart if they can answer questions: the pass a test, win
on \jeopardyp{}, or offer a witty retort in a debate.
%
This is not just a recent phenomenon: question answering is deeply embedded in
our culture.
%
In myth \OE{}dipus answered the riddle of the sphynx.
%
Good governance depends on civil service exams.
%
A trivia subculture has grown around answering all manner of questions.

What is new is that computers are now getting into the game of answering
questions.
%
From \abr{ibm} Watson on \jeopardy{} to Siri and Alexa in every home, the ability of
computers to answer is a common yardstick of how smart computers are.

This book is not a book about how to build \abr{ai} systems or how use natural
language processing to extract answers from Wikipedia.
%
There are plenty of tutorials that teach you how to do that.
%
Nor is it an insider's guide to the world of high-stakes educational testing,
trivia games, or licensing exams.

Unlike those books, the goal of this book is to draw \emph{connections}
between the two worlds of human and computer question answering.
%
The length of time computers have been answering questions well is shorter
than the tenure of beloved game show hosts.
%
However, the \abr{ai} boffins building these impressive systems have ignored
the hard-won lessons of humans' centuries of practice.

My argument is that we can better understand artificial intelligence by
looking at its ability to answer questions through this historical lens.
%
If we want to call a computer smart---indeed, smarter than a human---we should
make sure the competition is fair.
%
Moreover, writing and asking questions is an art that has been refined over
decades.

Part of the story I want to tell is how we got here.
%
And why little things like the decisions made at a British university
fifty years ago have locked the artificial intelligence community into
a particular way of evaluating whether a computer is smart.
%
These historical decisions have shaped the systems that we have today;
as a consequence, we are stuck with the crappy answers from our
smartphone.

But it didn't have to be this way!
%
The very definition of computing and artificial intelligence was based a
thought experiment that is closer to how computers and humans should interact
with each other.
%
However, that definition was too fanciful to really be implemented in
the lab.
%
I'll argue that we could do something closer to Alan Turing's vision,
and you can judge whether it's (still) too fanciful.

But I'll save my proposal for restructuring question answering
research after a thorough review of the history of question answering
(by both humans and computers).
%
This is not just a narrow question for those building question
answering systems.
%
I think that the way we interact with our smartphones and computers
will define the future of human--computer interaction and
thus the shape of the modern, \abr{ai}-infused economy.

\paragraph{Proposed Chapter Outline}

Proposed table of contentents follows.

\begin{enumerate}
        \item Question Answering Past

        \begin{enumerate*}
                \item {\bf Epic Questions}: Riddles and trivia have been with us for
                millenia.  One reason is that they're entertaining.  But it's
                more than that: the ability to ask and answer questions is
                close to godliness.  Through the lens of the Riddle of the
                Sphynx and Gestumblindi, this chapter examines what makes a
                good question in the eyes of the Gods.

              \item {\bf How Exams Saved Civilization}: Civil service
                exams broke the corruption and randomness of patronage
                and familial bonds.  By examining the reforms of Wu
                Zetian and the Pendelton Act, this chapter examines
                how information and competence---measured by questions
                asked in an exam---created meritocracy and social
                mobility.  The legacy of these exams built an
                understanding of what it means to write a ``fair'' and
                ``useful'' question, which has unfortunately been
                forgotten by \abr{ai} researchers.

        \end{enumerate*}

        \item Question Answering Present

        \begin{enumerate*}

        \item {\bf The Turing Test: A Game Show Pitch that Defined Artificial Intelligence}: Alan Turing's eponymous Turing
          Test is the most durable (but contentious) definition of
          what it means for a computer to be intelligent.  This
          chapter reviews how Alan Turing's postwar thought experiment
          of humans asking computers questions shaped the development
          of artificial intelligence.

        \item {\bf The Creation of Trivia's Gold Standard}: Just as
          Alan Turing's wartime exploits shaped \abr{ai}, this chapter
          charts the course of how a Canadian soldier created a
          revolution in how humans play trivia games (but these
          insights are ignored by the computer community).

        \item {\bf What Makes a Good Question}:\footnote{Published as a
        position piece at the Association of Computational Linguistics 2020.}
        Contrasting with the previous chapter, this chapter describes the best
        practices of human question answering---non-ambiguous, discriminative,
        and fair---and how these best practices grew out of decades of
        refinement.

      \item {\bf Watson on Jeopardy!: Unquestioned Answers from IBM's
          tour de force}: The most memorable computer to answer
        questions caused the public to think that question answering
        had been ``solved'', but that's not true.  This chapter breaks
        down why despite the hype and victory Watson's victory doesn't
        tell us that much about how well computers can answer
        questions.

        \item {\bf How Google, Alexa, and Siri Answer Questions Today}:
        This chapter reviews how computers answer questions: learning from
        example datasets and then recreate those examples from those datasets.
        This chapter critically examines how computers answer questions and
        how this can make computers seem smarter than they are.
        
        \item {\bf Cranfield vs. Manchester}:\footnote{Submitted as a short
        position piece to Empirical Methods in Natural Language Processing
        2021.} At a 1960s meeting in Cranfield, the computer science community
        embarked on a paradigm that downplayed the user of experts in defining
        systems.  This legacy, a shift from Alan Turing, still defines
        question answering today.

        \item {\bf What Leaderboards have Done to AI Research (and how
          trivia can save it)}: Computers compete on
        leaderboards, while humans compete at tournaments.  The latter has
        been optimized over decades to efficiently select the best human
        competitor, while leaderboards have not taken advantage of the tips
        and tricks that make tournaments for humans fun and informative.

        \end{enumerate*}

        \item Question Answering Future

        \begin{enumerate*}
        \item {\bf Science Fiction Question Answering}: Science fiction is the
        standard mode of predicting how humans and computers will interact in
        the future.  From Hal's questions in \textit{2001} to the Voigt-Kampff
        test in \textit{Blade Runner}, this chapter reflects on how plausible
        and what warnings science fiction offers for question answering.

        \item {\bf Human--Computer Centaurs}: This chapter looks at
        another \abr{ai} victory---chess---and how computers evolved into
        collaborators of humans rather than competitors.  How should computers
        and humans work together to answer hard questions?  The answer (and
        how we build \abr{ai} systems) will determine the future of the
        economy.

        \item {\bf Computer Game Shows of the Future}: Finally, we bring
        together two of the themes of the book: computers answering questions
        and game shows.  How could we use the gimick of the game show format
        to shape \abr{ai} into what would best serve society and help inform the
        public about the true state of \abr{ai} research?

        \end{enumerate*}

\end{enumerate}

\paragraph{Author Information}

Jordan Boyd-Graber is an associate professor at the University of Maryland.
He recieved a PhD from Princeton in 2010 in Computer Science and a bachelor's
degree in History from the California Institute of Technology.
%
His work has been recognized with ``best of'' awards at Intelligent User
Interfaces, Neural Information Processing Systems, the North American
Association for Computational Linguistics, and the Conference on Natural
Language Learning.
%
He also recieved a National Science Foundation \abr{career} award and the 2015
Karen Sp\"ark Jones Award.
%
He has published over a hundred peer-reviewed publications on interactive
machine learning, question answering, and exploring document collections.
%
With David Mimno and Yuening Hu, he wrote \emph{Applications of Topic Models}.

In addition to his academic research, Boyd-Graber is also active in the trivia
community: he founded the University of Colorado Quiz Bowl team, revived the
Princeton Graduate College Pub Quiz, coaches the national champion University
of Maryland Academic Quiz Team, and has lost on \jeopardyp{} (his
students are particularly fond of mocking his poor performance on the
video game category).

\paragraph{Book Audience}

The book is intended either for the general public to learn about humand and computer
question answering or for machine learning researchers to learn
how \emph{human} question answering can improve their computer question
answering systems.

\paragraph{Comparable Books}

\begin{itemize}
        \item Ken Jennings.  \textit{Braniac: Adventures in the Curious,
        Competitive, Compulsive World of Trivia Buffs}.  Villard, 2007.

        \item Mark T. Maybury.  \textit{New Directions in Question
        Answering}.  AAAI Press, 2004.

        \item Navin Sabharwal and Amit Agrawal.  \textit{Hands-on Question
        Answering Systems with BERT}.  A Press, 2021.
\end{itemize}


\paragraph{Additional Information and Specifications}

Estimated book contents:
\begin{itemize}
  \item 150 Single-spaced pages
  \item Two dozen diagrams / illustrations
  \item Draft to reviewers August 31, 2021, revisions/edits until
    January 31, 2021
\end{itemize}

Two chapters were submitted to conferences as position papers (one
accepted, one under review).  If accepted, the chapters will be
substantially revised to flow well with and complement other
chapters.

Sample chapter to follow.