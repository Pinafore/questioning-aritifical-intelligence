[
    {
      "label": "introduction",
      "summary": "Since this is a book about question answering, I should try to answer some questions to start things out: ``what's this book about'', ``who am I'', and ``who can use this book''."
    },
    {
      "label": "civilization",
      "summary": "Despite its central importance to \\abr{ai}, the concept of asking and answering questions is not new; it stretches back into our collective unconscious. Riddles and trivia have persisted for millennia not just because they're entertaining.  Rather, in many cultures, being able to ask and answer questions is close to godliness. From the Sphynx to Gestumblindi, this chapter examines how myths from multiple cultures connect answering questions to forming identity, unlocking the secrets of the universe, and gaining intelligence. Beyond mythis, in recorded history, asking questions helped combat corruption and nepotism, which were the downfall of multiple civilizations.  Indeed, the administration of carefully formed questions saved China and the United States via the reforms from Wu Zetian to the Pendelton Act.  Exams created meritocracy and social mobility while also building an understanding of what it means to write a ``fair'' and ``useful'' question.",
"sections": [
  {
    "label": "sphynx",
    "title": "The Riddle of the Sphynx: Questions that Define Destiny",
    "summary": "Introduces question answering through myth, including the Sphynx’s riddle as an early example of how questions can determine life, death, and human identity."
  },
  {
    "label": "socratic",
    "title": "Socratic Questions as a Teaching Tool",
    "summary": "Explores the Socratic method as a foundational model of question answering, where dialogue and probing questions serve as a tool for reasoning, learning, and uncovering hidden assumptions."
  },
  {
    "label": "gestumblindi",
    "title": "Only a God Would Ask Questions Like that, ``Gestumblindi''",
    "summary": "Examines the Old Norse riddle contest of Gestumblindi, highlighting how question answering has long been tied to wit, deception, and the boundary between human and divine knowledge."
  },
  {
    "label": "civil-service",
    "title": "How Exams Saved China and the US",
    "summary": "Shows how large-scale examination systems in imperial China and later the United States institutionalized question answering as a mechanism for governance, meritocracy, and social power."
  }
]
    },
    {
      "label": "turing",
      "summary": "In the 20th century, we turned the instinct of measuring worth through questions toward machines. In the 1950s, Alan Turing proposed a parlor game that would come to define artificial intelligence: could a wily interrogator discern whom they were talking to just through posing clever questions? The eponymous Turing Test is the most durable (but contentious) definition of what an intelligent computer is, but it is built on deception.",
      "sections": [
        {"label": "legacy", "title": "Turing's Legacy", "summary": "Why Turing is widely considered the father of computer science for multiple reasons and how he contributed to cloak and dagger spy operations."},
        {"label": "imitation", "title": "The Imitation Game", "summary": "A gay man in England---where homosexuality is criminalized---posits a game about the ability of someone to pass in different gender roles.  Why this is difficult lays the foundation for why computers still struggle to capture human nuance."},
        {"label": "failures", "title": "No, the Turing Test has not been Solved", "summary": "While many have claimed to pass the Turing Test, it hasn't happened yet.  We review some notable claims of passing the Turing Test and why they fell short, and why you shouldn't fall for the claims of charlatans."},
        {"label": "test", "title": "A Rigorous Test", "summary": "The ingredients that make a Turing Test legitimate."},
        {"label": "agi", "title": "General Artificial Intelligence", "summary": "How the holy grail of AI is defined and how the Turing Test can measure it."}
      ]
    },    
    {
  "label": "ir",
  "summary": "Modern search engines feel inevitable, but they emerged from a radical mid-century idea: that question answering systems could be evaluated scientifically using reusable test collections. While the Turing test is a \\emph{conceptual} measurement, this chapter traces the origins of the Cranfield paradigm's \\emph{empirical} measurement of machines' ability to answer questions.  It introduces precision and recall as the core metrics of retrieval, and shows how sparse term-based methods like tf-idf laid the groundwork for large-scale evaluation efforts at NIST and ultimately for web search as we know it. It also situates these retrieval advances within the broader trajectory of “good old fashioned” AI approaches to question answering. ! Irene comment: I think this needs to be more parallel to be a stronger contrast with the Manchester paradigm, and you need to explicitly say something like "the Cranfield paradigm ",
  "sections": [
    {
      "label": "cranfield",
      "title": "Cleverdon's Clever Collection",
      "summary": "Introduces Cyril Cleverdon’s Cranfield experiments, which replaced librarian-crafted indexing with a measurable framework for evaluating retrieval systems using shared document collections and relevance judgments."
    },
    {
      "label": "precision-recall",
      "title": "Is it Better to Find All the Good Things or for Everything Found to be Good?",
      "summary": "Presents the classic contingency-table view of retrieval evaluation, defining precision and recall as the fundamental tradeoff between finding relevant documents and avoiding irrelevant ones."
    },
    {
      "label": "tf-idf",
      "title": "Which Words Matter Most in your Index?",
      "summary": "Explains how term-frequency and inverse document frequency weighting, developed by Karen Spärck Jones, transformed Cranfield’s evaluation insights into practical retrieval methods that could scale beyond curated catalogs."
    },
    {
      "label": "gofai-qa",
      "title": "Old-Fashioned \\abr{ai}'s Search for Meaning",
	"summary": "Connects information retrieval to symbolic AI traditions, where question answering was framed as logical inference and knowledge representation rather than ranking documents by statistical similarity.  Discusses Green et al.'s 1979 Baseball system; Woods and Kaplan's 1972 LUNAR system; and Winograd's SHRDLU."
    },
    {
      "label": "nist",
      "title": "Making Hide and Seek (in Documents) a Professional Game",
      "summary": "Describes how NIST and the TREC competitions institutionalized the Cranfield paradigm, building larger test collections that enabled systematic progress and shaped the evaluation culture behind Google-era search."
    }
  ]
},
    {
        "label": "manchester",
        "summary": "While computer scientists were getting their feet wet answering questions in the wake of World War II, trivia enthusiasts were developing how to ask the perfect question.  This chapter outlines the conventions and processes of the highest form of question answering---in the biased opinion of the author---and argues for why many of these standards could be a part of creating data for computer question answering.  Previously published as \\citet{boyd-graber-20}.",
        "sections": [
            {"label": "name", "title": "Two British University Towns, Alike in Question Answering", "summary": "In contrast to the existing Cranfield Paradigm, we propose the Manchester paradigm, which creates probing questions to test specific skills of \\abr{ai} systems."},
            {"label": "qb", "title": "It Takes a Village to Ask a Question", "summary": "I review the Quizbowl competitions---whose University Challenge incarnation is one of the reasons we adopt the name Manchester paradigm---and how it reflects the incremental perfection of question answering for humans and what lessons (e.g., adjudication, focusing on discriminative questions) can transfer to computer question answering."},
	    {"label": "craft", "title": "The Craft of Question Writing", "summary": "A good trivia question should be unambiguous, discriminative, and test knowledge.  We review how these norms developed and are executed in crafting questions for humans."},
            {"label": "distinction", "title": "Ignore the Distinction at Your Peril", "summary": "Why the distinction between Cranfield and Manchester is relevant for understanding \\abr{ai} ability."},
	    {"label": "tournaments", "title": "Your Trivia Tournament is a Leaderboard", "summary": "Leaderboards in question answering are not fundamentally new: they are accuracy statistics that induce rankings, much like trivia tournaments. This section argues that QA evaluation can learn from decades of human competition norms about fairness, clarity, and discrimination."}
        ]
    },
      {
        "label": "watson",
          "summary": "It's been nearly a decade since IBM Watson crushed two puny humans on Jeopardy!.  Some people took that to mean that computers were definitively better than humans at trivia.  But that isn't the complete answer---this chapter, inspired by Jeopardy!'s gimmick of responding to answers to questions, questions some of the ``answers'' that emerged from IBM's tour de force.",
	  "sections": [
  {
    "label": "grand-challenge",
    "title": "Why IBM Chose Jeopardy! for a Grand Challenge",
    "summary": "Places Watson in the tradition of AI grand challenges such as Deep Blue and AlphaGo, emphasizing how these projects are designed to create cultural as well as technical shifts in the understanding of machine intelligence."
  },
  {
    "label": "how-watson-works",
    "title": "How Watson Works",
    "summary": "Introduces Watson as a statistical QA system built from retrieval, feature engineering, and probabilistic decision-making rather than modern neural generation."
  },
  {
    "label": "rigged",
    "title": "This Game Is Rigged, I Tell Ya!",
    "summary": "Presents the chapter’s central critique: Watson’s victory was not a clean scientific comparison, because the match conditions systematically advantaged the machine."
  },
	      {
		  "label": "strategy",
		  "title": "Two Nice Guys, One Computer with no Shame",
		  "summary": "Watson is not just a system that provides answers to questions.  It also needs to decide which questions to answer and whether to trust a guess.  How does it decide these, and how does that contribute to Watson's perceived human domination."

	      },
  {
    "label": "legacy",
    "title": "The Legacy of Watson",
    "summary": "Concludes that Watson marked both a milestone in question answering and a cautionary tale about evaluation, motivating the need for fairer benchmarks and clearer measures of progress."
  }
]
      },
      {
        "label": "formats",
        "summary": "How do you ask a question: via text, a picture, or a conversation?  This chapter reviews the different forms question answering can take and what complexities that can introduce.",
        "sections": [
  {
    "label": "task",
    "title": "Task vs. Format",
    "summary": "Introduces the distinction between what question answering systems are trying to accomplish (the task) and the external shape that QA takes in practice (the format). The chapter argues that many of the hardest problems in QA emerge not from new tasks, but from new formats that demand context, interaction, or multiple kinds of evidence."
  },
  {
    "label": "question",
    "title": "What is a Question?",
    "summary": "Explores how questions vary far beyond short factoid prompts, ranging from multihop queries to conversational follow-ups and long-form explanations. The section emphasizes that defining what counts as a “question” is itself a modeling challenge, because real information needs are often underspecified, contextual, and evolving."
  },
  {
    "label": "evidence",
    "title": "Where do you get an Answer?",
    "summary": "Discusses the role of evidence in QA systems, contrasting settings where answers are directly extractable from a single passage with formats that require gathering multiple supporting documents. The section frames retrieval, linking, and reasoning over evidence as central bottlenecks for moving beyond one-hop machine reading."
  },
  {
    "label": "conversation",
    "title": "What did you Mean?",
    "summary": "Focuses on conversational question answering, where users ask sequences of context-dependent questions that require memory, clarification, and ambiguity resolution. The section surveys datasets like QuAC, CoQA, and CANARD, and highlights the evaluation difficulty: conversational success depends on interaction quality, not just matching a reference answer."
  },
  {
    "label": "multimodal",
    "title": "A Picture is worth a Thousand Questions",
    "summary": "Extends QA beyond text to multimodal settings, where systems must answer questions about images, video, or audio. The section introduces visual QA datasets such as VQA and VizWiz, explains how transformers incorporate visual objects as inputs, and argues that evaluation becomes harder when answers are not naturally grounded in text spans."
  },
  {
    "label": "language",
    "title": "Questions Beyond English",
    "summary": "Briefly motivates the need for QA systems that work across languages and cultural contexts, where named entities, scripts, and question conventions differ substantially. The section notes that multilingual QA raises both technical challenges in retrieval and deeper questions about what constitutes an acceptable answer across linguistic communities."
  },
  {
    "label": "skills",
    "title": "The Skills Needed to Answer Questions",
    "summary": "Synthesizes the diverse capabilities required for robust QA: retrieval, coreference resolution, multistep reasoning, and dialog management. The section emphasizes that QA is not a single problem but a bundle of interacting skills."
  },
  {
    "label": "taxonomy",
    "title": "A Taxonomy of Question Answering",
    "summary": "Concludes by organizing QA formats into a broader taxonomy, showing how multihop, conversational, long-form, multimodal, and spoken QA each stress different assumptions about questions, evidence, and evaluation. The section argues that progress in QA depends on matching models and metrics to the true complexity of human information seeking."
  }
]
      },
      {
        "label": "datasets",
        "summary": "The clich\\'e is that data are the new oil, powering \\abr{ai}.  Fortunately, because humans naturally ask questions, there are many datasets that we can find `for free'.  However, these datasets still come at a cost: many of these datasets have inherent problems (e.g., ambiguities and false presuppositions) or oddities (only talking about American men) that make them difficult to use for question answering.  This chapter discusses these datasets that have formed the foundation of much of modern \\abr{ai}. Of course, if you can't find the data you need, you can build it yourself.  But this is not always a perfect solution.  This chapter also discusses the datasets that people have built and the new problems that this can create.",
          "sections": [
              {"label": "crowdsourcing", "title": "Constructed Datasets from Crowdsourcing", "summary": "The SQuAD dataset helped create the first wave of question answering.  It also paved the way for other constructed datasets built from workers on the web."},      
              {"label": "nq", "title": "Natural Questions Five Years Later", "summary": "Google's Natural Questions dataset represents the most realistic dataset to question answering, but is it a good way to train computers?"},
              {"label": "experts", "title": "Turning to Experts", "summary": "Another option is to repurpose datasets created by experts designed to test humans."}
        ]        
      }, 
      {
        "label": "methods",
        "summary": "Having discussed how we create datasets that can teach computers how to answer questions, we now explore how those datasets can train modern computer methods.",
        "sections": [
          {"label": "kb", "title": "Modern Knowledge", "summary": "This section reviews how we can turn natural language queries into actionable queries in databases."},
          {"label": "mr", "title": "Johnny 5 Can't Read", "summary": "However, putting information in a database is difficult and time-consuming\\dots not all information is in a database (indeed, some information defies strict database schemas).  Thus, we need to teach computers how to read.  This chapter reviews the process of ``machine reading'', where computers find information in a large text corpus and then extracts the answer from it."},
          {"label": "deep-retrieval", "title": "A Vector in a Haystack", "summary": "As deep learning became practical, the field has moved from representing text with discrete words to continuous vectors.  This is also the case for question answering.  This chapter reviews how these representations can help us find relevant answers to questions."},
          {"label": "generation", "title": "We Couldn't Find an Answer, so we had to Generate One", "summary": "The deep learning revolution not just helps us find answers but to generate them.  This chapter talks about the promise and peril of these approaches: how we can synthesize much richer information, make stuff up, encourage these agents to align to our wishes using RLHF, and how it's hard to tell if an answer is any good."}
        ]
      },
      {
        "label": "leaderboards",
        "summary": "How do we know how smart a machine is?  Like a human, we typically give it a test; the difference is that tests for computers are called `leaderboards'. This chapter talks about the pros and cons of leaderboards and how some of the methods used to analyze human standardized tests can help us understand the strengths and weaknesses of not just computer question answering specifically but \\abr{ai} generally.  This also marks the reappearance of item response theory, which can help make deciding the smartest computer more efficient.  Previously published as \\citet{Rodriguez-21:leaderboards} and \\citet{mgor-24}.",
          "sections": [
              {"label": "cheat", "title":"When 21 questions are worthless", "summary": "We compare how a trivia cheating scandal extends to AI leaderboards.  Questions have been memorized, the question askers offer tells and signals so that a Clever Hans can still answer the question without understanding, and exams don't necessarily measure what they claim to."},
      	      {"label": "hype", "title": "The Hype and Confusion of Leaderboards", "summary": "When a new model comes out, there is usually a hullaballoo about how it's the `best ever'.  But how do you know?  This is a job for a leaderboard, but these aren't always up to the task."},
	      {"label": "zermelo", "title": "The Balance Between Skill and Difficulty", "summary": "In the 1920s, Ernst Zermelo came up with a formula to determine, given a chess tournament who was the best player.  This formula, building on logistic regression, became the foundation for understanding not just Elo rankings but political ideology and students' skill when taking the SAT. It also gave birth the Bradley-Terry models needed for RLHF."},
	      {"label": "irt-leaderboards", "title": "Is this Question Bad or am I just Stupid?", "summary": "A leaderboard is like a game show.  If you keep every player's score, it can give you clues not just about how good they are but whether questions are good or not.  And once you know what the best questions are, you can deploy them more judiciously."},
	      {"label": "comparative-skill", "title": "Sure, you can add fifteen digit numbers, but can you identify a \textit{Simpsons} plot from a vague description?", "summary": "If we think about skill as a vector (i.e., I can be good at math but bad at chemistry), then we can have a more nuanced comparison between those who answer questions.  This allows for a clearer comparison between computer agents but also between humans and computers: computers are good at raw fact retrieval, but humans are better at matching abstract patterns and making inductive ``leaps''."},	      
              {"label": "adversarial", "title": "Easy for Humans, Hard for Computers", "summary": "If existing datasets and game show appearances aren't enough to tell whether humans or computers are better at answering questions, what can we do?  While there is an argument for focusing on natural data, modern language models are changing not just what is possible computationally but changing the language itself.  Thus, we need to select examples specifically to challenge computers. These examples are called \\emph{adversarial examples}, and this chapter presents how to gather them and how they can reveal the strengths and weaknesses of computer question answering."},
	      {"label": "cooperation", "title": "Augmenting Humans, not Replacing Them", "summary": "But perhaps we shouldn't be measuring humans and computers on the same yardstick to see which is better.  A more realistic measurement that better measures how humans will be working with \\abr{ai} is seeing how well humans and computers can work together to answer a question."}
        ]
      },
    {"label": "bad",
     "summary": "This chapter discusses how questions can be imperfect and how both humans and computers deal with those imperfect questions.  Navigating an imperfect question requires more skill than answering a perfectly posed one.  Not only must the answerer have the requisite information about the correct answer, they must exclude incorrect interpretations and navigate how to decide what the asker actually needs.",
     "sections": [
	 {"label": "ambiguity", "title": "What's the Capital of Georgia?", "summary": "When you hear a question, there can be multiple interpretations.  How to resolve those interpretations often requires as much mental agility as actually answering the question."},
	 {"label": "presup", "title": "When did you stop beating your wife?", "summary": "Sometimes questions assume something that they shouldn't.  Detecting these ``false presuppositions'' is tricky enough for humans, but modern \\abr{ai} is particularly gullible and often plays along when it shouldn't."},
	 {"label": "answers", "title": "What city is the setting of Crime and Punishment?", "summary": "Even if a question is well-formed, sometimes the problem lies with the candidate answers or detecting when an answer is correct.  This is a problem that seems easy at first, but the devil is in the details."},	 
	 {"label": "navigating", "title": "What do you do with a broken question?", "summary": "One of the challenges humans are able to overcome with relative ease is navigating broken questions to get to the answer that the asker actually wants.  We discuss what strategies humans use and how computers can learn from them."}
     ],
     "title": "How to Navigate Bad Questions"},    
      {
        "label": "gameshow",
          "summary": "The year is 2025, and there's a new gameshow that not only showcases the most advanced \\abr{ai} available but also keeps the public informed about the limitations and struggles of current technology.  This chapter outlines the ten seasons of the show and how it tracks the development of machine intelligence, leading to passing the Turing Test.",
	  "sections": [
	      {"label": "knowledge", "title": "Where there's room for improvement", "summary": "One of the core arguments of the book is that AI can learn from trivia and game shows.  What if we put our money where our mouth is and followed through?  How could we improve AI by taking the best of both worlds and made it entertaining?"},
	      {"label": "culture", "title": "Moving beyond White American Men", "summary": "Many datasets---both Cranfield and Manchester---overwhelmingly talk about white American men.  Is that a problem, and if so, how might we improve dataset representation and \\abr{ai}'s understanding of other people and places?"},
	      {"label": "social", "title": "Some Rules aren't Written", "summary": "One of the reasons I'm still a fan of the Turing test is that it put implicit social norms as a key part of the test.  This has not been incorporated into major AI evaluations, but it should."},
	      {"label": "roadmap", "title": "Making the Turing Test Real (and Fun)", "summary": "We conclude the chapter with a roadmap for how an AI game show could grow and evolve into a replicable, entertaining, and effective test of AI intellectual and social intelligence that Alan Turing would recognize as a descendant of his thought experiment."}
	  ]},
      {
        "label": "sci_fi",
        "summary": "While the book began by discussing mythical question answering and how it helped the ancients grapple with a changing and uncertain world, today's fictions represent our contemporary attempts to understand the advent of intelligent computers.  This chapter reviews human--computer question answering in 2001, Star Trek, The Terminator, Blade Runner, Futurama, and what these depictions reveal about both human conceptions of artificial intelligence and how they might shape future deployments of question answering.",
        "sections": [
          {"label": "presentations", "title": "Computer, speculate", "summary": "A review of the role of QA in science fiction: Voigt-Kampf, Star Trek, and Futurama have all used humans questioning computers to test how human or intelligent they are.  Do these portrayals offer lessons for the real world?"},
            {"label": "fake-fears", "title": "p(Doom)", "summary": "Beyond the imagining of science fiction, what are the actual downsides to the widespread availability and deployment of `intelligent' \\abr{ai}?  This chapter talks about \\abr{ai}'s ability to amplify existing negative \\emph{human} tendencies in responding to questions and how---while dangerous and worthy of mitigation---don't demand us to halt the development of modern \\abr{ai}."},
	    {"label": "valid-fears", "title": "Fear of a Bot Planet", "summary": "I conclude the book with a discussion of what the risks of widespread AI deployment actually are and what questions we should be asking of people who are advocating for increasing the role of AI."}
        ]
      }
] 
